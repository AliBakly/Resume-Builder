
\begin{rSubsection}
{\href{https://github.com/AliBakly/EPFLLaMA}{\underline{EPFLLaMA: A Lightweight LLM Finetuned on EPFL Curriculum} \href{Project Link}{\raisebox{-0.1\height}\faExternalLink }}}{}{}{}{}{}{}

\item Led development of EPFLLaMA, a specialized language model for STEM education.
\item Managed the entire dataset creation process, including data scraping, cleaning, and annotation.
\item Implemented advanced fine-tuning techniques including Supervised Fine-Tuning (\textbf{SFT}) and Direct Preference Optimization (\textbf{DPO}) to enhance model performance and reduce bias.
\item Applied Parameter-Efficient Fine-Tuning (\textbf{PEFT}) methods, specifically Low-Rank Adaptation (\textbf{LoRA}).%, to optimize model training and improve efficiency.
\item Developed a specialized model for Multiple-Choice Question Answering, improving accuracy by 100\% compared to baseline models in STEM-related tasks.
\item Implemented \textbf{quantization} techniques, reducing model size by 50\% while maintaining performance, demonstrating skills in model optimization for practical applications.
\item Leveraged \textbf{Python} and \textbf{PyTorch} along with specialized libraries like \textbf{Transformers}, \textbf{TRL}, and \textbf{Unsloth} for model development, training, and optimization.
\end{rSubsection}
\vspace{-8pt}